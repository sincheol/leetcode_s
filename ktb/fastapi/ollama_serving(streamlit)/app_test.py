import streamlit as st
import ollama
from PIL import Image

'''
streamlitì—ì„œ ollamaì„œë²„ë¡œ ì±„íŒ…ê³¼ ê°™ì€ ë°ì´í„°ë¥¼ ë³´ë‚´ì„œ ì£¼ê³  ë°›ê³  ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ëŠ” í˜•ì‹
fastapiì™€ëŠ” ì°¨ì›ì´ ë‹¤ë¥¸ í¸ë¦¬í•¨.. í•˜ì§€ë§Œ í‘œí˜„ì´ ì œí•œì ì„..

+++++++++++++++++++++++++++++++++++++=
ìš°ë¦¬ëŠ” postmanì„ ì‚¬ìš©í•´ì„œ apië¡œ ìš”ì²­ì„ ì ë•ŒëŠ” base64ë¡œ encodingì´ í•„ìš”í–ˆìŒ
ì—¬ê¸°ì„œëŠ” Raw bytesë¡œë§Œ ë³€í™˜í•´ì„œ ë³´ë‚´ì£¼ë©´ ollamaë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ httpìš”ì²­ì„ ë³´ë‚´ê¸° ì „ ì¸ì½”ë”©ì„ ìˆ˜í–‰í•´ jsonë³¸ë¬¸ì— í¬í•¨
'''


#1. í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë‚´ ì»´í“¨í„° ì† GPT", page_icon="ğŸ’¬")
st.title("ğŸ’¬ ë‚˜ë§Œì˜ ë¡œì»¬ GPT (with Gemma3)")

#ëª¨ë¸ id
model_id = "gemma3:4b"

#2. ì´ë¯¸ì§€ ì˜¬ë¦´ ì‚¬ì´ë“œë°” ì„¤ì •
with st.sidebar:
    st.header('ì´ë¯¸ì§€ ì—…ë¡œë“œ')

    st.caption(f"Runs on {model_id}")

    uploaded_file = st.file_uploader("ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ë ¤ë©´ ì—…ë¡œë“œí•˜ì‡¼", type = ['png', 'jpg', 'jpeg'])

    if uploaded_file:
        st.info("ì´ë¯¸ì§€ ë¶„ì„")
        image = Image.open(uploaded_file)
        st.image(image, caption = 'uploaded image', use_container_width = True)

#3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ëŒ€í™” ê¸°ë¡ ì €ì¥ì†Œ)
# Streamlitì€ ë§¤ë²ˆ ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ë¯€ë¡œ, ëŒ€í™” ê¸°ë¡ì´ ë‚ ì•„ê°€ì§€ ì•Šê²Œ session_stateì— ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.
if "messages" not in st.session_state:
    st.session_state["messages"] = []

#4. ì´ì „ ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥
# (ìƒˆë¡œê³ ì¹¨ ë˜ì–´ë„ ì´ì „ ëŒ€í™”ê°€ ë‚¨ì•„ìˆê²Œ í•¨)
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

#5. ì‚¬ìš©ì ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”..."):
    #5-1. ì‚¬ìš©ìì˜ ì…ë ¥ì„ í™”ë©´ì— í‘œì‹œí•˜ê³  ê¸°ë¡ì— ì €ì¥
    with st.chat_message("user"):
        st.markdown(prompt)
        if uploaded_file:
            st.image(uploaded_file,width = 200)
    
    #payloadê°€ ë˜ê² ì§€?
    text_message = {"role": "user", "content": prompt}

    #5-2. AIì˜ ë‹µë³€ì„ ìƒì„±í•˜ê³  í™”ë©´ì— í‘œì‹œ (ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)
    with st.chat_message("assistant"):
        message_placeholder = st.empty() # ë¹ˆ ê³µê°„ì„ ë¯¸ë¦¬ ë§Œë“¦
        full_response = ""
        
        if uploaded_file:
            text_message['images'] = [uploaded_file.getvalue()]
            #imageê°€ ìˆì„ ë•Œ í˜„ì¬ ì§ˆë¬¸ë§Œ
            #ìš°ì„  image ë°ì´í„°(base64)ëŠ” ìš©ëŸ‰ì´ í…ìŠ¤íŠ¸ì™€ ë¹„êµê°€ ì•ˆë ì •ë„ë¡œ í¼
            #ë©€í‹°í„´ ìƒí™©ì‹œ ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ì™€ ê°™ì€ ë°ì´í„°ê°€ ë„ˆë¬´ ìŒ“ì—¬ ëª¨ë¸ì´ ì œëŒ€ë¡œ ê¸°ì–µëª»í•˜ê±°ë‚˜ ì´ìƒí•´ì§€ëŠ” ë¬¸ì œê°€ ìƒê¸¸ ìˆ˜ ìˆìŒ
            #ìš°ë¦¬ëŠ” í˜„ì¬ ì…ë ¥í•œ ëŒ€í™”ì™€ ì´ë¯¸ì§€ì—ë§Œ ì§‘ì¤‘í•´ í•´ê²°..
            #geminiì™€ ê°™ì€ ëª¨ë¸ë“¤ì€ Long Context Window(ë§ì€ í† í° ì²˜ë¦¬ê°€ëŠ¥), ê·¸ ì•ˆì—ì„œ ì •ë³´ë¥¼ ì°¾ëŠ” ì •í™•ë„(Attention/Retrieval)ë¥¼ ë†’ì´ëŠ” ë°©ì‹ìœ¼ë¡œ í•´ê²°


        st.session_state.messages.append(text_message)

        messages_to_send = st.session_state.messages

        #Ollamaì—ê²Œ ëŒ€í™” ê¸°ë¡ ì „ì²´ë¥¼ ë³´ë‚´ì„œ ë‹µë³€ ìš”ì²­
        #stream=True: ê¸€ìë¥¼ í•œ ê¸€ìì”© íƒ€ìê¸°ì²˜ëŸ¼ ë°›ê¸° ìœ„í•¨
        response = ollama.chat(
            model=model_id,
            messages=messages_to_send,
            stream=True
        )
        
        #Ollamaì˜ ë‹µë³€
        #í•œ ê¸€ìì”© ë°›ì•„ì„œ í™”ë©´ì— ì—…ë°ì´íŠ¸
        for chunk in response:
            token = chunk['message']['content']
            full_response += token
            message_placeholder.markdown(full_response + "â–Œ") # ì»¤ì„œ íš¨ê³¼

        #ì™„ë£Œë˜ë©´ ì»¤ì„œ ì œê±°í•˜ê³  ìµœì¢… í…ìŠ¤íŠ¸ í‘œì‹œ
        message_placeholder.markdown(full_response)
    
    #5-3. AIì˜ ë‹µë³€ë„ ê¸°ë¡ì— ì €ì¥ (ê·¸ë˜ì•¼ ë‹¤ìŒ ëŒ€í™” ë•Œ ê¸°ì–µí•¨)
    st.session_state.messages.append({"role": "assistant", "content": full_response})