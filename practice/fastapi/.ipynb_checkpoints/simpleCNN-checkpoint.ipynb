{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b4388f1-7eac-4d82-8e4c-9c8388bab79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmallCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Input:  [B, 3, 224, 224]\n",
    "    Output: [B, 2] (logits for ['dog','cat'])\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes: int = 2, p_drop: float = 0.2):\n",
    "        super().__init__()\n",
    "        def block(cin, cout):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(cin, cout, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(cout),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(cout, cout, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(cout),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2)  # downsample by 2\n",
    "            )\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            block(3,   32),   # 224 -> 112\n",
    "            nn.Dropout(p_drop),\n",
    "            block(32,  64),   # 112 -> 56\n",
    "            nn.Dropout(p_drop),\n",
    "            block(64, 128),   # 56  -> 28\n",
    "            nn.Dropout(p_drop),\n",
    "            block(128, 256),  # 28  -> 14\n",
    "            nn.Dropout(p_drop),\n",
    "        )\n",
    "        # global average pooling to 1×1\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Linear(256, num_classes)\n",
    "\n",
    "        # Kaiming init for convs\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)          # [B, 256, 14, 14]\n",
    "        x = self.gap(x)               # [B, 256, 1, 1]\n",
    "        x = torch.flatten(x, 1)       # [B, 256]\n",
    "        x = self.classifier(x)        # [B, 2] (logits)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e050b3-3e40-4c78-8d09-b929854cbc21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=0.7056 | val_acc=0.5000\n",
      "Epoch 02 | train_loss=0.5527 | val_acc=0.6000\n",
      "Epoch 03 | train_loss=0.5290 | val_acc=0.5000\n",
      "Epoch 04 | train_loss=0.5125 | val_acc=0.5000\n",
      "Epoch 05 | train_loss=0.4686 | val_acc=0.5000\n",
      "Epoch 06 | train_loss=0.4505 | val_acc=0.5000\n",
      "Epoch 07 | train_loss=0.3688 | val_acc=0.5000\n",
      "Epoch 08 | train_loss=0.3661 | val_acc=0.5000\n",
      "Epoch 09 | train_loss=0.3525 | val_acc=0.5000\n",
      "Epoch 10 | train_loss=0.3184 | val_acc=0.5000\n",
      "Epoch 11 | train_loss=0.3839 | val_acc=0.5000\n",
      "Epoch 12 | train_loss=0.3422 | val_acc=0.5000\n",
      "Epoch 13 | train_loss=0.3023 | val_acc=0.5000\n",
      "Epoch 14 | train_loss=0.2680 | val_acc=0.6000\n",
      "Epoch 15 | train_loss=0.2188 | val_acc=0.6000\n",
      "Epoch 16 | train_loss=0.2910 | val_acc=0.6000\n",
      "Epoch 17 | train_loss=0.2999 | val_acc=0.6000\n",
      "Epoch 18 | train_loss=0.1917 | val_acc=0.6000\n",
      "Epoch 19 | train_loss=0.1980 | val_acc=0.6000\n",
      "Epoch 20 | train_loss=0.2278 | val_acc=0.6000\n",
      "Best val_acc = 0.6000\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision import transforms\n",
    "\n",
    "# from models.cnn import SmallCNN\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Match FastAPI preprocessing: ToTensor -> [-1, 1]\n",
    "\n",
    "train_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "val_tf = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "# Directory layout:\n",
    "# data/\n",
    "#   train/\n",
    "#     dog/*.jpg\n",
    "#     cat/*.jpg\n",
    "#   val/\n",
    "#     dog/*.jpg\n",
    "#     cat/*.jpg\n",
    "train_ds = datasets.ImageFolder('data/train', transform=train_tf)\n",
    "val_ds   = datasets.ImageFolder('data/val',   transform=val_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "model = SmallCNN(num_classes=2).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "best_acc, epochs = 0.0, 20\n",
    "for epoch in range(1, epochs+1):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running += loss.item() * x.size(0)\n",
    "\n",
    "    # validation\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            pred = logits.argmax(dim=1)\n",
    "            correct += (pred == y).sum().item()\n",
    "            total += y.numel()\n",
    "    val_acc = correct / total\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={running/len(train_ds):.4f} | val_acc={val_acc:.4f}\")\n",
    "\n",
    "    # keep best\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        os.makedirs('pts', exist_ok=True)\n",
    "        # torch.save(model.state_dict(), 'pts/model.pt') \n",
    "        # torch.save(model, './pts/model.pt')\n",
    "print(f\"Best val_acc = {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db489d-1eec-435c-a367-1e4946bd8582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3d16962-2c91-4894-a336-47cb65f06a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxscript in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (0.5.6)\n",
      "Requirement already satisfied: ml_dtypes in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxscript) (0.5.3)\n",
      "Requirement already satisfied: numpy in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxscript) (1.26.4)\n",
      "Requirement already satisfied: onnx_ir<2,>=0.1.12 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxscript) (0.1.12)\n",
      "Requirement already satisfied: onnx>=1.16 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxscript) (1.19.1)\n",
      "Requirement already satisfied: packaging in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxscript) (25.0)\n",
      "Requirement already satisfied: typing_extensions>=4.10 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxscript) (4.15.0)\n",
      "Requirement already satisfied: protobuf>=4.25.1 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnx>=1.16->onnxscript) (6.33.0)\n",
      "Requirement already satisfied: onnxruntime in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (1.23.2)\n",
      "Requirement already satisfied: coloredlogs in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxruntime) (25.9.23)\n",
      "Requirement already satisfied: numpy>=1.21.6 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: packaging in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxruntime) (25.0)\n",
      "Requirement already satisfied: protobuf in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxruntime) (6.33.0)\n",
      "Requirement already satisfied: sympy in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from onnxruntime) (1.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages (from sympy->onnxruntime) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install onnxscript\n",
    "!pip install onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02a29a3d-7fcd-4d61-9c17-a4cb8edc3bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.onnx] Obtain model graph for `SmallCNN([...]` with `torch.export.export(..., strict=False)`...\n",
      "[torch.onnx] Obtain model graph for `SmallCNN([...]` with `torch.export.export(..., strict=False)`... ✅\n",
      "[torch.onnx] Run decomposition...\n",
      "[torch.onnx] Run decomposition... ✅\n",
      "[torch.onnx] Translate the graph into ONNX...\n",
      "[torch.onnx] Translate the graph into ONNX... ✅\n",
      "Applied 16 of general pattern rewrite rules.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ONNXProgram(\n",
       "    model=\n",
       "        <\n",
       "            ir_version=10,\n",
       "            opset_imports={'': 20},\n",
       "            producer_name='pytorch',\n",
       "            producer_version='2.9.0',\n",
       "            domain=None,\n",
       "            model_version=None,\n",
       "        >\n",
       "        graph(\n",
       "            name=main_graph,\n",
       "            inputs=(\n",
       "                %\"x\"<FLOAT,[1,3,224,224]>\n",
       "            ),\n",
       "            outputs=(\n",
       "                %\"linear\"<FLOAT,[1,2]>\n",
       "            ),\n",
       "            initializers=(\n",
       "                %\"features.0.0.weight\"<FLOAT,[32,3,3,3]>{Tensor(...)},\n",
       "                %\"features.0.3.weight\"<FLOAT,[32,32,3,3]>{Tensor(...)},\n",
       "                %\"features.2.0.weight\"<FLOAT,[64,32,3,3]>{Tensor(...)},\n",
       "                %\"features.2.3.weight\"<FLOAT,[64,64,3,3]>{Tensor(...)},\n",
       "                %\"features.4.0.weight\"<FLOAT,[128,64,3,3]>{Tensor(...)},\n",
       "                %\"features.4.3.weight\"<FLOAT,[128,128,3,3]>{Tensor(...)},\n",
       "                %\"features.6.0.weight\"<FLOAT,[256,128,3,3]>{Tensor(...)},\n",
       "                %\"features.6.3.weight\"<FLOAT,[256,256,3,3]>{Tensor(...)},\n",
       "                %\"classifier.weight\"<FLOAT,[2,256]>{TorchTensor(...)},\n",
       "                %\"classifier.bias\"<FLOAT,[2]>{TorchTensor<FLOAT,[2]>(Parameter containing: tensor([-0.0220, -0.0395], requires_grad=True), name='classifier.bias')},\n",
       "                %\"val_81\"<INT64,[2]>{Tensor<INT64,[2]>(array([-1, -2]), name='val_81')},\n",
       "                %\"val_85\"<INT64,[2]>{Tensor<INT64,[2]>(array([  1, 256]), name='val_85')},\n",
       "                %\"x_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"relu_bias\"<FLOAT,[32]>{Tensor(...)},\n",
       "                %\"max_pool2d_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"relu_2_bias\"<FLOAT,[64]>{Tensor(...)},\n",
       "                %\"max_pool2d_1_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"relu_4_bias\"<FLOAT,[128]>{Tensor(...)},\n",
       "                %\"max_pool2d_2_bias\"<FLOAT,[256]>{Tensor(...)},\n",
       "                %\"relu_6_bias\"<FLOAT,[256]>{Tensor(...)}\n",
       "            ),\n",
       "        ) {\n",
       "             0 |  # node_Conv_107\n",
       "                  %\"getitem\"<FLOAT,[1,32,224,224]> ⬅️ ::Conv(%\"x\", %\"features.0.0.weight\"{...}, %\"x_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             1 |  # node_relu\n",
       "                  %\"relu\"<FLOAT,[1,32,224,224]> ⬅️ ::Relu(%\"getitem\")\n",
       "             2 |  # node_Conv_109\n",
       "                  %\"getitem_3\"<FLOAT,[1,32,224,224]> ⬅️ ::Conv(%\"relu\", %\"features.0.3.weight\"{...}, %\"relu_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             3 |  # node_relu_1\n",
       "                  %\"relu_1\"<FLOAT,[1,32,224,224]> ⬅️ ::Relu(%\"getitem_3\")\n",
       "             4 |  # node_max_pool2d\n",
       "                  %\"max_pool2d\"<FLOAT,[1,32,112,112]>, %\"\"<?,?> ⬅️ ::MaxPool(%\"relu_1\") {storage_order=0, ceil_mode=0, pads=(0, 0, 0, 0), kernel_shape=(2, 2), strides=(2, 2), dilations=(1, 1), auto_pad='NOTSET'}\n",
       "             5 |  # node_Conv_111\n",
       "                  %\"getitem_6\"<FLOAT,[1,64,112,112]> ⬅️ ::Conv(%\"max_pool2d\", %\"features.2.0.weight\"{...}, %\"max_pool2d_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             6 |  # node_relu_2\n",
       "                  %\"relu_2\"<FLOAT,[1,64,112,112]> ⬅️ ::Relu(%\"getitem_6\")\n",
       "             7 |  # node_Conv_113\n",
       "                  %\"getitem_9\"<FLOAT,[1,64,112,112]> ⬅️ ::Conv(%\"relu_2\", %\"features.2.3.weight\"{...}, %\"relu_2_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "             8 |  # node_relu_3\n",
       "                  %\"relu_3\"<FLOAT,[1,64,112,112]> ⬅️ ::Relu(%\"getitem_9\")\n",
       "             9 |  # node_max_pool2d_1\n",
       "                  %\"max_pool2d_1\"<FLOAT,[1,64,56,56]>, %\"\"<?,?> ⬅️ ::MaxPool(%\"relu_3\") {storage_order=0, ceil_mode=0, pads=(0, 0, 0, 0), kernel_shape=(2, 2), strides=(2, 2), dilations=(1, 1), auto_pad='NOTSET'}\n",
       "            10 |  # node_Conv_115\n",
       "                  %\"getitem_12\"<FLOAT,[1,128,56,56]> ⬅️ ::Conv(%\"max_pool2d_1\", %\"features.4.0.weight\"{...}, %\"max_pool2d_1_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            11 |  # node_relu_4\n",
       "                  %\"relu_4\"<FLOAT,[1,128,56,56]> ⬅️ ::Relu(%\"getitem_12\")\n",
       "            12 |  # node_Conv_117\n",
       "                  %\"getitem_15\"<FLOAT,[1,128,56,56]> ⬅️ ::Conv(%\"relu_4\", %\"features.4.3.weight\"{...}, %\"relu_4_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            13 |  # node_relu_5\n",
       "                  %\"relu_5\"<FLOAT,[1,128,56,56]> ⬅️ ::Relu(%\"getitem_15\")\n",
       "            14 |  # node_max_pool2d_2\n",
       "                  %\"max_pool2d_2\"<FLOAT,[1,128,28,28]>, %\"\"<?,?> ⬅️ ::MaxPool(%\"relu_5\") {storage_order=0, ceil_mode=0, pads=(0, 0, 0, 0), kernel_shape=(2, 2), strides=(2, 2), dilations=(1, 1), auto_pad='NOTSET'}\n",
       "            15 |  # node_Conv_119\n",
       "                  %\"getitem_18\"<FLOAT,[1,256,28,28]> ⬅️ ::Conv(%\"max_pool2d_2\", %\"features.6.0.weight\"{...}, %\"max_pool2d_2_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            16 |  # node_relu_6\n",
       "                  %\"relu_6\"<FLOAT,[1,256,28,28]> ⬅️ ::Relu(%\"getitem_18\")\n",
       "            17 |  # node_Conv_121\n",
       "                  %\"getitem_21\"<FLOAT,[1,256,28,28]> ⬅️ ::Conv(%\"relu_6\", %\"features.6.3.weight\"{...}, %\"relu_6_bias\"{...}) {group=1, pads=(1, 1, 1, 1), strides=(1, 1), auto_pad='NOTSET', dilations=(1, 1)}\n",
       "            18 |  # node_relu_7\n",
       "                  %\"relu_7\"<FLOAT,[1,256,28,28]> ⬅️ ::Relu(%\"getitem_21\")\n",
       "            19 |  # node_max_pool2d_3\n",
       "                  %\"max_pool2d_3\"<FLOAT,[1,256,14,14]>, %\"\"<?,?> ⬅️ ::MaxPool(%\"relu_7\") {storage_order=0, ceil_mode=0, pads=(0, 0, 0, 0), kernel_shape=(2, 2), strides=(2, 2), dilations=(1, 1), auto_pad='NOTSET'}\n",
       "            20 |  # node_mean\n",
       "                  %\"mean\"<FLOAT,[1,256,1,1]> ⬅️ ::ReduceMean(%\"max_pool2d_3\", %\"val_81\"{[-1, -2]}) {noop_with_empty_axes=0, keepdims=1}\n",
       "            21 |  # node_view\n",
       "                  %\"view\"<FLOAT,[1,256]> ⬅️ ::Reshape(%\"mean\", %\"val_85\"{[1, 256]}) {allowzero=1}\n",
       "            22 |  # node_linear\n",
       "                  %\"linear\"<FLOAT,[1,2]> ⬅️ ::Gemm(%\"view\", %\"classifier.weight\"{...}, %\"classifier.bias\"{[-0.02202565036714077, -0.03954116627573967]}) {beta=1.0, transB=1, alpha=1.0, transA=0}\n",
       "            return %\"linear\"<FLOAT,[1,2]>\n",
       "        }\n",
       "\n",
       "\n",
       "    ,\n",
       "    exported_program=\n",
       "        ExportedProgram:\n",
       "            class GraphModule(torch.nn.Module):\n",
       "                def forward(self, p_features_0_0_weight: \"f32[32, 3, 3, 3]\", p_features_0_1_weight: \"f32[32]\", p_features_0_1_bias: \"f32[32]\", p_features_0_3_weight: \"f32[32, 32, 3, 3]\", p_features_0_4_weight: \"f32[32]\", p_features_0_4_bias: \"f32[32]\", p_features_2_0_weight: \"f32[64, 32, 3, 3]\", p_features_2_1_weight: \"f32[64]\", p_features_2_1_bias: \"f32[64]\", p_features_2_3_weight: \"f32[64, 64, 3, 3]\", p_features_2_4_weight: \"f32[64]\", p_features_2_4_bias: \"f32[64]\", p_features_4_0_weight: \"f32[128, 64, 3, 3]\", p_features_4_1_weight: \"f32[128]\", p_features_4_1_bias: \"f32[128]\", p_features_4_3_weight: \"f32[128, 128, 3, 3]\", p_features_4_4_weight: \"f32[128]\", p_features_4_4_bias: \"f32[128]\", p_features_6_0_weight: \"f32[256, 128, 3, 3]\", p_features_6_1_weight: \"f32[256]\", p_features_6_1_bias: \"f32[256]\", p_features_6_3_weight: \"f32[256, 256, 3, 3]\", p_features_6_4_weight: \"f32[256]\", p_features_6_4_bias: \"f32[256]\", p_classifier_weight: \"f32[2, 256]\", p_classifier_bias: \"f32[2]\", b_features_0_1_running_mean: \"f32[32]\", b_features_0_1_running_var: \"f32[32]\", b_features_0_1_num_batches_tracked: \"i64[]\", b_features_0_4_running_mean: \"f32[32]\", b_features_0_4_running_var: \"f32[32]\", b_features_0_4_num_batches_tracked: \"i64[]\", b_features_2_1_running_mean: \"f32[64]\", b_features_2_1_running_var: \"f32[64]\", b_features_2_1_num_batches_tracked: \"i64[]\", b_features_2_4_running_mean: \"f32[64]\", b_features_2_4_running_var: \"f32[64]\", b_features_2_4_num_batches_tracked: \"i64[]\", b_features_4_1_running_mean: \"f32[128]\", b_features_4_1_running_var: \"f32[128]\", b_features_4_1_num_batches_tracked: \"i64[]\", b_features_4_4_running_mean: \"f32[128]\", b_features_4_4_running_var: \"f32[128]\", b_features_4_4_num_batches_tracked: \"i64[]\", b_features_6_1_running_mean: \"f32[256]\", b_features_6_1_running_var: \"f32[256]\", b_features_6_1_num_batches_tracked: \"i64[]\", b_features_6_4_running_mean: \"f32[256]\", b_features_6_4_running_var: \"f32[256]\", b_features_6_4_num_batches_tracked: \"i64[]\", x: \"f32[1, 3, 224, 224]\"):\n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d: \"f32[1, 32, 224, 224]\" = torch.ops.aten.conv2d.default(x, p_features_0_0_weight, None, [1, 1], [1, 1]);  x = p_features_0_0_weight = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d, p_features_0_1_weight, p_features_0_1_bias, b_features_0_1_running_mean, b_features_0_1_running_var, 0.1, 1e-05);  conv2d = p_features_0_1_weight = p_features_0_1_bias = b_features_0_1_running_mean = b_features_0_1_running_var = None\n",
       "                    getitem: \"f32[1, 32, 224, 224]\" = _native_batch_norm_legit_no_training[0];  _native_batch_norm_legit_no_training = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu: \"f32[1, 32, 224, 224]\" = torch.ops.aten.relu.default(getitem);  getitem = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_1: \"f32[1, 32, 224, 224]\" = torch.ops.aten.conv2d.default(relu, p_features_0_3_weight, None, [1, 1], [1, 1]);  relu = p_features_0_3_weight = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_1 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_1, p_features_0_4_weight, p_features_0_4_bias, b_features_0_4_running_mean, b_features_0_4_running_var, 0.1, 1e-05);  conv2d_1 = p_features_0_4_weight = p_features_0_4_bias = b_features_0_4_running_mean = b_features_0_4_running_var = None\n",
       "                    getitem_3: \"f32[1, 32, 224, 224]\" = _native_batch_norm_legit_no_training_1[0];  _native_batch_norm_legit_no_training_1 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_1: \"f32[1, 32, 224, 224]\" = torch.ops.aten.relu.default(getitem_3);  getitem_3 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d: \"f32[1, 32, 112, 112]\" = torch.ops.aten.max_pool2d.default(relu_1, [2, 2], [2, 2]);  relu_1 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone: \"f32[1, 32, 112, 112]\" = torch.ops.aten.clone.default(max_pool2d);  max_pool2d = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_2: \"f32[1, 64, 112, 112]\" = torch.ops.aten.conv2d.default(clone, p_features_2_0_weight, None, [1, 1], [1, 1]);  clone = p_features_2_0_weight = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_2 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_2, p_features_2_1_weight, p_features_2_1_bias, b_features_2_1_running_mean, b_features_2_1_running_var, 0.1, 1e-05);  conv2d_2 = p_features_2_1_weight = p_features_2_1_bias = b_features_2_1_running_mean = b_features_2_1_running_var = None\n",
       "                    getitem_6: \"f32[1, 64, 112, 112]\" = _native_batch_norm_legit_no_training_2[0];  _native_batch_norm_legit_no_training_2 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_2: \"f32[1, 64, 112, 112]\" = torch.ops.aten.relu.default(getitem_6);  getitem_6 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_3: \"f32[1, 64, 112, 112]\" = torch.ops.aten.conv2d.default(relu_2, p_features_2_3_weight, None, [1, 1], [1, 1]);  relu_2 = p_features_2_3_weight = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_3 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_3, p_features_2_4_weight, p_features_2_4_bias, b_features_2_4_running_mean, b_features_2_4_running_var, 0.1, 1e-05);  conv2d_3 = p_features_2_4_weight = p_features_2_4_bias = b_features_2_4_running_mean = b_features_2_4_running_var = None\n",
       "                    getitem_9: \"f32[1, 64, 112, 112]\" = _native_batch_norm_legit_no_training_3[0];  _native_batch_norm_legit_no_training_3 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_3: \"f32[1, 64, 112, 112]\" = torch.ops.aten.relu.default(getitem_9);  getitem_9 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_1: \"f32[1, 64, 56, 56]\" = torch.ops.aten.max_pool2d.default(relu_3, [2, 2], [2, 2]);  relu_3 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_1: \"f32[1, 64, 56, 56]\" = torch.ops.aten.clone.default(max_pool2d_1);  max_pool2d_1 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_4: \"f32[1, 128, 56, 56]\" = torch.ops.aten.conv2d.default(clone_1, p_features_4_0_weight, None, [1, 1], [1, 1]);  clone_1 = p_features_4_0_weight = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_4 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_4, p_features_4_1_weight, p_features_4_1_bias, b_features_4_1_running_mean, b_features_4_1_running_var, 0.1, 1e-05);  conv2d_4 = p_features_4_1_weight = p_features_4_1_bias = b_features_4_1_running_mean = b_features_4_1_running_var = None\n",
       "                    getitem_12: \"f32[1, 128, 56, 56]\" = _native_batch_norm_legit_no_training_4[0];  _native_batch_norm_legit_no_training_4 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_4: \"f32[1, 128, 56, 56]\" = torch.ops.aten.relu.default(getitem_12);  getitem_12 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_5: \"f32[1, 128, 56, 56]\" = torch.ops.aten.conv2d.default(relu_4, p_features_4_3_weight, None, [1, 1], [1, 1]);  relu_4 = p_features_4_3_weight = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_5 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_5, p_features_4_4_weight, p_features_4_4_bias, b_features_4_4_running_mean, b_features_4_4_running_var, 0.1, 1e-05);  conv2d_5 = p_features_4_4_weight = p_features_4_4_bias = b_features_4_4_running_mean = b_features_4_4_running_var = None\n",
       "                    getitem_15: \"f32[1, 128, 56, 56]\" = _native_batch_norm_legit_no_training_5[0];  _native_batch_norm_legit_no_training_5 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_5: \"f32[1, 128, 56, 56]\" = torch.ops.aten.relu.default(getitem_15);  getitem_15 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_2: \"f32[1, 128, 28, 28]\" = torch.ops.aten.max_pool2d.default(relu_5, [2, 2], [2, 2]);  relu_5 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_2: \"f32[1, 128, 28, 28]\" = torch.ops.aten.clone.default(max_pool2d_2);  max_pool2d_2 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_6: \"f32[1, 256, 28, 28]\" = torch.ops.aten.conv2d.default(clone_2, p_features_6_0_weight, None, [1, 1], [1, 1]);  clone_2 = p_features_6_0_weight = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_6 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_6, p_features_6_1_weight, p_features_6_1_bias, b_features_6_1_running_mean, b_features_6_1_running_var, 0.1, 1e-05);  conv2d_6 = p_features_6_1_weight = p_features_6_1_bias = b_features_6_1_running_mean = b_features_6_1_running_var = None\n",
       "                    getitem_18: \"f32[1, 256, 28, 28]\" = _native_batch_norm_legit_no_training_6[0];  _native_batch_norm_legit_no_training_6 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_6: \"f32[1, 256, 28, 28]\" = torch.ops.aten.relu.default(getitem_18);  getitem_18 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/conv.py:548 in forward, code: return self._conv_forward(input, self.weight, self.bias)\n",
       "                    conv2d_7: \"f32[1, 256, 28, 28]\" = torch.ops.aten.conv2d.default(relu_6, p_features_6_3_weight, None, [1, 1], [1, 1]);  relu_6 = p_features_6_3_weight = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:193 in forward, code: return F.batch_norm(\n",
       "                    _native_batch_norm_legit_no_training_7 = torch.ops.aten._native_batch_norm_legit_no_training.default(conv2d_7, p_features_6_4_weight, p_features_6_4_bias, b_features_6_4_running_mean, b_features_6_4_running_var, 0.1, 1e-05);  conv2d_7 = p_features_6_4_weight = p_features_6_4_bias = b_features_6_4_running_mean = b_features_6_4_running_var = None\n",
       "                    getitem_21: \"f32[1, 256, 28, 28]\" = _native_batch_norm_legit_no_training_7[0];  _native_batch_norm_legit_no_training_7 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/activation.py:144 in forward, code: return F.relu(input, inplace=self.inplace)\n",
       "                    relu_7: \"f32[1, 256, 28, 28]\" = torch.ops.aten.relu.default(getitem_21);  getitem_21 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/pooling.py:226 in forward, code: return F.max_pool2d(\n",
       "                    max_pool2d_3: \"f32[1, 256, 14, 14]\" = torch.ops.aten.max_pool2d.default(relu_7, [2, 2], [2, 2]);  relu_7 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/dropout.py:73 in forward, code: return F.dropout(input, self.p, self.training, self.inplace)\n",
       "                    clone_3: \"f32[1, 256, 14, 14]\" = torch.ops.aten.clone.default(max_pool2d_3);  max_pool2d_3 = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/pooling.py:1500 in forward, code: return F.adaptive_avg_pool2d(input, self.output_size)\n",
       "                    mean: \"f32[1, 256, 1, 1]\" = torch.ops.aten.mean.dim(clone_3, [-1, -2], True);  clone_3 = None\n",
       "            \n",
       "                     # File: /var/folders/l3/zb21wy1j0s13v2zm9zlf8ckm0000gn/T/ipykernel_15830/493322811.py:47 in forward, code: x = torch.flatten(x, 1)       # [B, 256]\n",
       "                    view: \"f32[1, 256]\" = torch.ops.aten.view.default(mean, [1, 256]);  mean = None\n",
       "            \n",
       "                     # File: /Users/jeonghyunpark/miniconda3/envs/env_ktbai/lib/python3.10/site-packages/torch/nn/modules/linear.py:134 in forward, code: return F.linear(input, self.weight, self.bias)\n",
       "                    linear: \"f32[1, 2]\" = torch.ops.aten.linear.default(view, p_classifier_weight, p_classifier_bias);  view = p_classifier_weight = p_classifier_bias = None\n",
       "                    return (linear,)\n",
       "            \n",
       "        Graph signature: \n",
       "            # inputs\n",
       "            p_features_0_0_weight: PARAMETER target='features.0.0.weight'\n",
       "            p_features_0_1_weight: PARAMETER target='features.0.1.weight'\n",
       "            p_features_0_1_bias: PARAMETER target='features.0.1.bias'\n",
       "            p_features_0_3_weight: PARAMETER target='features.0.3.weight'\n",
       "            p_features_0_4_weight: PARAMETER target='features.0.4.weight'\n",
       "            p_features_0_4_bias: PARAMETER target='features.0.4.bias'\n",
       "            p_features_2_0_weight: PARAMETER target='features.2.0.weight'\n",
       "            p_features_2_1_weight: PARAMETER target='features.2.1.weight'\n",
       "            p_features_2_1_bias: PARAMETER target='features.2.1.bias'\n",
       "            p_features_2_3_weight: PARAMETER target='features.2.3.weight'\n",
       "            p_features_2_4_weight: PARAMETER target='features.2.4.weight'\n",
       "            p_features_2_4_bias: PARAMETER target='features.2.4.bias'\n",
       "            p_features_4_0_weight: PARAMETER target='features.4.0.weight'\n",
       "            p_features_4_1_weight: PARAMETER target='features.4.1.weight'\n",
       "            p_features_4_1_bias: PARAMETER target='features.4.1.bias'\n",
       "            p_features_4_3_weight: PARAMETER target='features.4.3.weight'\n",
       "            p_features_4_4_weight: PARAMETER target='features.4.4.weight'\n",
       "            p_features_4_4_bias: PARAMETER target='features.4.4.bias'\n",
       "            p_features_6_0_weight: PARAMETER target='features.6.0.weight'\n",
       "            p_features_6_1_weight: PARAMETER target='features.6.1.weight'\n",
       "            p_features_6_1_bias: PARAMETER target='features.6.1.bias'\n",
       "            p_features_6_3_weight: PARAMETER target='features.6.3.weight'\n",
       "            p_features_6_4_weight: PARAMETER target='features.6.4.weight'\n",
       "            p_features_6_4_bias: PARAMETER target='features.6.4.bias'\n",
       "            p_classifier_weight: PARAMETER target='classifier.weight'\n",
       "            p_classifier_bias: PARAMETER target='classifier.bias'\n",
       "            b_features_0_1_running_mean: BUFFER target='features.0.1.running_mean' persistent=True\n",
       "            b_features_0_1_running_var: BUFFER target='features.0.1.running_var' persistent=True\n",
       "            b_features_0_1_num_batches_tracked: BUFFER target='features.0.1.num_batches_tracked' persistent=True\n",
       "            b_features_0_4_running_mean: BUFFER target='features.0.4.running_mean' persistent=True\n",
       "            b_features_0_4_running_var: BUFFER target='features.0.4.running_var' persistent=True\n",
       "            b_features_0_4_num_batches_tracked: BUFFER target='features.0.4.num_batches_tracked' persistent=True\n",
       "            b_features_2_1_running_mean: BUFFER target='features.2.1.running_mean' persistent=True\n",
       "            b_features_2_1_running_var: BUFFER target='features.2.1.running_var' persistent=True\n",
       "            b_features_2_1_num_batches_tracked: BUFFER target='features.2.1.num_batches_tracked' persistent=True\n",
       "            b_features_2_4_running_mean: BUFFER target='features.2.4.running_mean' persistent=True\n",
       "            b_features_2_4_running_var: BUFFER target='features.2.4.running_var' persistent=True\n",
       "            b_features_2_4_num_batches_tracked: BUFFER target='features.2.4.num_batches_tracked' persistent=True\n",
       "            b_features_4_1_running_mean: BUFFER target='features.4.1.running_mean' persistent=True\n",
       "            b_features_4_1_running_var: BUFFER target='features.4.1.running_var' persistent=True\n",
       "            b_features_4_1_num_batches_tracked: BUFFER target='features.4.1.num_batches_tracked' persistent=True\n",
       "            b_features_4_4_running_mean: BUFFER target='features.4.4.running_mean' persistent=True\n",
       "            b_features_4_4_running_var: BUFFER target='features.4.4.running_var' persistent=True\n",
       "            b_features_4_4_num_batches_tracked: BUFFER target='features.4.4.num_batches_tracked' persistent=True\n",
       "            b_features_6_1_running_mean: BUFFER target='features.6.1.running_mean' persistent=True\n",
       "            b_features_6_1_running_var: BUFFER target='features.6.1.running_var' persistent=True\n",
       "            b_features_6_1_num_batches_tracked: BUFFER target='features.6.1.num_batches_tracked' persistent=True\n",
       "            b_features_6_4_running_mean: BUFFER target='features.6.4.running_mean' persistent=True\n",
       "            b_features_6_4_running_var: BUFFER target='features.6.4.running_var' persistent=True\n",
       "            b_features_6_4_num_batches_tracked: BUFFER target='features.6.4.num_batches_tracked' persistent=True\n",
       "            x: USER_INPUT\n",
       "    \n",
       "            # outputs\n",
       "            linear: USER_OUTPUT\n",
       "    \n",
       "        Range constraints: {}\n",
       "\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.onnx\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "torch.onnx.export(model, dummy_input, \"./pts/model_from_torch.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fff75d-be4a-4b0a-9bb5-d78856911e36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ktbai",
   "language": "python",
   "name": "env_ktbai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
